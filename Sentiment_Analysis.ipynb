{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrmBXMhXcaGCgKODtO0nMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristianokunas/Sentiment_Analysis/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuj8IAK-NMD0",
        "colab_type": "text"
      },
      "source": [
        "Imports and downloads the stopwords data\n",
        "===="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKPy3eK9dEPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuC8wTIHdIPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtLeai_edKtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==1.14.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMuL4aIdOt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmN_QcQz2-9d",
        "colab_type": "text"
      },
      "source": [
        "# Etapa 1 - Importando as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIJ9GMLpdTDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM  # , Bidirectional\n",
        "# from keras.layers import Input\n",
        "from keras.models import Sequential #, Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from nltk.corpus import stopwords\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIJGfGzK3lTM",
        "colab_type": "text"
      },
      "source": [
        "# Etapa 2 - Conectando com o Drive e acessando os arquivos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYAMXqQedXmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oG-TxjMdYk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/Lab/Data Science/Sentiment Analysis/IMDB/github/imdb.zip\"\n",
        "zip_object = zipfile.ZipFile(file=path, mode=\"r\")\n",
        "zip_object.extractall(\"/content/drive/My Drive/Lab/Data Science/Sentiment Analysis/IMDB\")\n",
        "zip_object.close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Miq-R9lZ5xGU",
        "colab_type": "text"
      },
      "source": [
        "# Etapa 3 - Acessando a base de dados com as reviews "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckB0VLLpddUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carrega o arquivo de dados .csv\n",
        "data = pd.read_csv('/content/drive/My Drive/Lab/Data Science/Sentiment Analysis/IMDB/imdb.csv')\n",
        "data.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEhzCSv5Eb-u",
        "colab_type": "text"
      },
      "source": [
        "# Etapa 4 - Pr√©-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eUlMAiadh3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "# O model sera exportado para este arquivo\n",
        "filename = '/content/drive/My Drive/Lab/Data Science/Sentiment Analysis/model_saved.h5'\n",
        "# numero de iteracoes\n",
        "epochs = 10  # email - 150\n",
        "# numero de amostras a serem utilizadas em cada atualizacao do gradiente - numero de instancias\n",
        "batch_size = 32  # email - 10\n",
        "# separa % para teste do modelo\n",
        "test_dim = 0.20\n",
        "# Quantidade maxima de palavras para manter no vocabulario\n",
        "max_fatures = 5000\n",
        "# dimensao de saida da camada Embedding\n",
        "embed_dim = 128\n",
        "# Tamanho maximo das sentencas\n",
        "max_sequence_length = 300\n",
        "\n",
        "\n",
        "# metodo para limpar as strings - tirar conteudo que nao agrega\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    cleanr = re.compile('<.*?>')\n",
        "\n",
        "    string = re.sub(r'\\d+', '', string)\n",
        "    string = re.sub(cleanr, '', string)\n",
        "    string = re.sub(\"'\", '', string)\n",
        "    string = re.sub(r'\\W+', ' ', string)\n",
        "    string = string.replace('_', '')\n",
        "\n",
        "    return string.strip().lower()\n",
        "\n",
        "# Metodo para preparar os dados de treino e teste\n",
        "# carrega csv, limpa as strings e remove as stop_wors\n",
        "# Realiza a tokenizacao\n",
        "def prepare_data(data):\n",
        "    data = data[['text', 'sentiment']]\n",
        "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "    data['text'] = data['text'].apply(lambda x: clean_str(x))\n",
        "    data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = []\n",
        "    for row in data['text'].values:\n",
        "        word_list = text_to_word_sequence(row)\n",
        "        no_stop_words = [w for w in word_list if not w in stop_words]\n",
        "        no_stop_words = \" \".join(no_stop_words)\n",
        "        text.append(no_stop_words)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    X = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "    X = pad_sequences(X, maxlen=max_sequence_length)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    Y = pd.get_dummies(data['sentiment']).values\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_dim, random_state=42)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test, word_index, tokenizer\n",
        "\n",
        "\n",
        "# chama metodo para preparar dados\n",
        "X_train, X_test, Y_train, Y_test, word_index, tokenizer = prepare_data(data)\n",
        "\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)\n",
        "\n",
        "# Cria o modelo\n",
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_fatures, embed_dim, input_length=max_sequence_length))\n",
        "    model.add(LSTM(embed_dim, dropout=0.2, recurrent_dropout=0.2, name=\"lstm\"))\n",
        "    model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
        "    model.add(Dense(8, init='uniform', activation='relu'))\n",
        "    model.add(Dense(2, init='uniform', activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Criacao do modelo\n",
        "model = model()\n",
        "\n",
        "# compilacao do modelo\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Treinamento da rede neural\n",
        "# Verifica se existe um modelo treinado\n",
        "# True = treina a rede e salva o modelo\n",
        "# False =\n",
        "if os.path.exists('./{}'.format(filename)):\n",
        "    model.load_weights('./{}'.format(filename))\n",
        "else:\n",
        "    hist = model.fit(\n",
        "        X_train,\n",
        "        Y_train,\n",
        "        validation_data=(X_test, Y_test),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size)\n",
        "\n",
        "    model.save_weights(filename)\n",
        "\n",
        "# Avaliando o modelo\n",
        "scores = model.evaluate(X_test, Y_test, verbose=0, batch_size=batch_size)\n",
        "print(\"Acc: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "\n",
        "while True:\n",
        "    sentence = input(\"input> \")\n",
        "\n",
        "    if sentence == \"exit\":\n",
        "        break\n",
        "\n",
        "    new_text = [sentence]\n",
        "    new_text = tokenizer.texts_to_sequences(new_text)\n",
        "\n",
        "    new_text = pad_sequences(new_text, maxlen=max_sequence_length, dtype='int32', value=0)\n",
        "\n",
        "    sentiment = model.predict(new_text, batch_size=1, verbose=2)[0]\n",
        "\n",
        "    if (np.argmax(sentiment) == 0):\n",
        "        pred_proba = \"%.2f%%\" % (sentiment[0] * 100)\n",
        "        print(\"negativo => \", pred_proba)\n",
        "    elif (np.argmax(sentiment) == 1):\n",
        "        pred_proba = \"%.2f%%\" % (sentiment[1] * 100)\n",
        "        print(\"positivo => \", pred_proba)\n",
        "\n",
        "    # press Q on keyboard to exit\n",
        "    # if Key(25) & 0xFF == ord('q'):\n",
        "    # break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmXwA5ooEAqu",
        "colab_type": "text"
      },
      "source": [
        "# Final - Unmount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqsyKrc6dlYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}